{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib notebook\n",
    "\n",
    "np.random.seed(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[str(i) for i in p_list]\n",
    "' '.join(p_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Data/train.csv')\n",
    "train.index=train.Webpage_id\n",
    "test = pd.read_csv('./Data/test.csv')\n",
    "test.index=test.Webpage_id\n",
    "print(\"Training samples = \", train.shape[0])\n",
    "print(\"Test samples = \", test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train,pd.get_dummies(train[['Tag']],prefix=['Tag'])],axis=1)\n",
    "train.drop(['Tag'],axis=1,inplace=True)\n",
    "test.drop(['Domain'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows =train.shape[0]\n",
    "labels = train[['Tag_clinicalTrials', 'Tag_conferences','Tag_forum', 'Tag_guidelines','Tag_news', 'Tag_others', 'Tag_profile','Tag_publication', 'Tag_thesis']] \n",
    "train.drop(['Tag_clinicalTrials', 'Tag_conferences','Tag_forum', 'Tag_guidelines','Tag_news', 'Tag_others', 'Tag_profile','Tag_publication', 'Tag_thesis','Domain'], axis=1, inplace=True)\n",
    "test_id = test.pop('Webpage_id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null values in training data\", train.isnull().sum(), sep=\"\\n\")\n",
    "print(\"Null values in testing data\", test.isnull().sum(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test],sort=True)\n",
    "data.drop(['Webpage_id'],axis=1,inplace=True)\n",
    "del train\n",
    "del test\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modified_data=pd.DataFrame(columns=data.columns)\n",
    "chunksize=10000\n",
    "for html in pd.read_csv('./Data/html_data.csv',index_col=False,infer_datetime_format=False,chunksize=chunksize):\n",
    "    html.index=html.Webpage_id\n",
    "    html['data']=html['Html'].apply(lambda i: BeautifulSoup(i,'html.parser'))\n",
    "    html['data']=html['data'].apply(lambda i: i.find_all(\"p\"))\n",
    "    html['data']=html['data'].apply(lambda i: [j.string for j in i])\n",
    "    html['data']=html['data'].apply(lambda i: [str(j) for j in i])\n",
    "    html['data']=html['data'].apply(lambda i: ' '.join(i))\n",
    "\n",
    "    data=pd.concat([data,html['data']],axis=1)\n",
    "    data['Url']=data['Url']+data['data']\n",
    "    data.drop(['data'],axis=1,inplace=True)\n",
    "    temp=data[~pd.isna(data.Url)]\n",
    "\n",
    "    modified_data=pd.concat([modified_data,temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_input(comment):\n",
    "# remove the extra spaces at the end.\n",
    "    comment = comment.strip()\n",
    "# lowercase to avoid difference between 'hate', 'HaTe'\n",
    "    comment = comment.lower()\n",
    "# remove the escape sequences. \n",
    "    comment = re.sub('[\\s0-9]',' ', comment)\n",
    "# Use nltk's word tokenizer to split the sentence into words. It is better than the 'split' method.\n",
    "    words = nltk.word_tokenize(comment)\n",
    "# removing the commonly used words.\n",
    "    #words = [word for word in words if not word in stop_words and len(word) > 2]\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    comment = ' '.join(words)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAMPLE PREPROCESSING\")\n",
    "print(\"\\nOriginal comment: \", data.Url.iloc[0], sep='\\n')\n",
    "print(\"\\nProcessed comment: \", preprocess_input(data.Url.iloc[0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Url = data.Url.apply(lambda row: preprocess_input(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.1, max_df=0.7, \n",
    "                       analyzer='char',\n",
    "                       ngram_range=(1, 3),\n",
    "                       strip_accents='unicode',\n",
    "                       sublinear_tf=True,\n",
    "                       max_features=5000\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[train_rows:]\n",
    "train = data[:train_rows]\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = vect.fit(train.Url)\n",
    "train = vect.transform(train.Url)\n",
    "test = vect.transform(test.Url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training feature set = ', train.shape)\n",
    "print('Testing feature set = ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cols = ['Tag_clinicalTrials', 'Tag_conferences','Tag_forum', 'Tag_guidelines','Tag_news', 'Tag_others', 'Tag_profile','Tag_publication', 'Tag_thesis']\n",
    "y_pred = pd.read_csv('./Data/sample_submission.csv')\n",
    "\n",
    "for c in cols:\n",
    "    clf = LogisticRegression(C=4, solver='sag',D)\n",
    "    clf.fit(train, labels[c])\n",
    "    y_pred[c] = clf.predict_proba(test)[:,1]\n",
    "    score = np.mean(cross_val_score(clf, train, labels[c], scoring='roc_auc', cv=5))\n",
    "    print(\"ROC_AUC score for\", c, \"=\",  score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred['Tag']=y_pred[['Tag_clinicalTrials', 'Tag_conferences','Tag_forum', \\\n",
    "                      'Tag_guidelines','Tag_news', 'Tag_others', \\\n",
    "                      'Tag_profile','Tag_publication', 'Tag_thesis']].idxmax(axis=1)\n",
    "y_pred['Tag']=y_pred['Tag'].apply(lambda i: re.sub('Tag_','',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[['Webpage_id','Tag']].to_csv('Linear_Regression_C4_Feature8000.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THis is iteration no: 1\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 2\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 3\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 4\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 5\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 6\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 7\n",
      "shape of df: (10000, 2)\n",
      "THis is iteration no: 8\n",
      "shape of df: (9345, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chunksize=10000\n",
    "i=0\n",
    "for html in pd.read_csv('./Data/html_data.csv',index_col=False,infer_datetime_format=False,chunksize=chunksize):\n",
    "    html.index=html.Webpage_id\n",
    "    print('THis is iteration no:',i+1)\n",
    "    print(\"shape of df:\",html.shape)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
